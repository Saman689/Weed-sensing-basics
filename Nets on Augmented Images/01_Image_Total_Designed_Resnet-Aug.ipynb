{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this notebook we investigate a designed Resnet network on augmented image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "from torch.autograd import Function, Variable\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import copy\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "import time as time\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking whether the GPU is active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.cudnn.enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path(\"/home/saman/Saman/data/Image_Data01/\")\n",
    "train_path = PATH / 'train' / 'Total'\n",
    "valid_path = PATH / 'valid' / 'Total'\n",
    "test_path = PATH / 'test' / 'Total'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Num_Filter1=16\n",
    "Num_Filter2=64 \n",
    "Ker_Sz1=5 \n",
    "Ker_Sz2=5\n",
    "\n",
    "learning_rate= 0.0001\n",
    "\n",
    "Dropout= 0.2\n",
    "BchSz= 1\n",
    "EPOCH= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss calculator\n",
    "criterion = nn.CrossEntropyLoss()   # cross entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining resnet classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(x)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv3(x)\n",
    "        out = self.bn3(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv4(out)\n",
    "        out = self.bn4(out)\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, Num_Filter1 , Num_Filter2, Ker_Sz1, Ker_Sz2, num_classes=2):\n",
    "        self.in_channels = Num_Filter2\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, out_channels=Num_Filter1, kernel_size=Ker_Sz1, stride=1,\n",
    "                               padding=int((Ker_Sz1-1)/2),bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(Num_Filter1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        \n",
    "        self.conv2 = nn.Conv2d(Num_Filter1, Num_Filter2, kernel_size=Ker_Sz2, stride=1,\n",
    "                               padding=int((Ker_Sz2-1)/2),bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(Num_Filter2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        \n",
    "        self.layer1 = self._make_layer(block, Num_Filter2, layers[0])\n",
    "        self.layer2 = self._make_layer(block, Num_Filter2, layers[1], stride=1)\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(7, stride=1, padding=1)\n",
    "        self.fc = nn.Linear(12616704* block.expansion, num_classes)\n",
    "        \n",
    "        # Self initiation weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freading number of parameter in our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_num_params(model):\n",
    "    TotalParam=0\n",
    "    for param in list(model.parameters()):\n",
    "        print(\"Individual parameters are:\")\n",
    "        nn=1\n",
    "        for size in list(param.size()):\n",
    "            print(size)\n",
    "            nn = nn*size\n",
    "        print(\"Total parameters: {}\" .format(param.numel()))\n",
    "        TotalParam += nn\n",
    "    print('-' * 10)\n",
    "    print(\"Sum of all Parameters is: {}\" .format(TotalParam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_params(model):\n",
    "    TotalParam=0\n",
    "    for param in list(model.parameters()):\n",
    "        nn=1\n",
    "        for size in list(param.size()):\n",
    "            nn = nn*size\n",
    "        TotalParam += nn\n",
    "    return TotalParam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and validation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer,  Dropout, learning_rate,  BATCHSIZE, num_epochs):\n",
    "        print(str(datetime.now()).split('.')[0], \"Starting training and validation...\\n\")\n",
    "        print(\"====================Data and Hyperparameter Overview====================\\n\")\n",
    "        print(\"Number of training examples: {} , Number of validation examples: {} \\n\".format(len(train_data), len(valid_data)))\n",
    "              \n",
    "        print(\"Dropout:{:,.2f}, Learning rate: {:,.5f} \" \n",
    "              .format( Dropout, learning_rate ))        \n",
    "        print(\"Batch size: {}, Number of epochs: {} \" \n",
    "              .format(BATCHSIZE, num_epochs)) \n",
    "        \n",
    "        print(\"Number of parameter in the model: {}\". format(get_num_params(model)))\n",
    "              \n",
    "        print(\"================================Results...==============================\\n\")\n",
    "\n",
    "        since = time.time()  #record the beginning time\n",
    "\n",
    "        best_model = model\n",
    "        best_acc = 0.0\n",
    "        acc_vect =[]   \n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            for i, (images, labels) in enumerate(train_loader):   \n",
    "                images = Variable(images).cuda()\n",
    "                labels = Variable(labels).cuda()\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(images)            # model output\n",
    "                loss = criterion(outputs, labels)  # cross entropy loss\n",
    "\n",
    "                # Trying binary cross entropy\n",
    "                #loss = criterion(torch.max(outputs.data, 1), labels)\n",
    "                #loss = torch.nn.functional.binary_cross_entropy(outputs, labels)\n",
    "                \n",
    "                \n",
    "\n",
    "                # Backward and optimize\n",
    "                optimizer.zero_grad()             # clear gradients for this training step\n",
    "                loss.backward()                   # backpropagation, compute gradients\n",
    "                optimizer.step()                  # apply gradients\n",
    "\n",
    "                if (i+1) % 1000 == 0:               # Reporting the loss and progress every 50 step\n",
    "                    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                               .format(epoch+1, num_epochs, i+1, len(train_loader), loss.item()))\n",
    "\n",
    "            model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                for images, labels in valid_loader:\n",
    "                    images = Variable(images).cuda()\n",
    "                    labels = Variable(labels).cuda()\n",
    "                    \n",
    "                    outputs = model(images)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss += loss.item()\n",
    "\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "\n",
    "                epoch_loss= loss / total\n",
    "                epoch_acc = 100 * correct / total\n",
    "                acc_vect.append(epoch_acc)\n",
    "\n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model = copy.deepcopy(model)\n",
    "\n",
    "                print('Validation accuracy and loss of the model on  {} images: {} %, {:.5f}'\n",
    "                      .format(len(valid_data), 100 * correct / total, loss))\n",
    "\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in train_loader:\n",
    "                images = Variable(images).cuda()\n",
    "                labels = Variable(labels).cuda()\n",
    "                \n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss += loss.item()\n",
    "\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "            epoch_loss= loss / total\n",
    "            epoch_acc = 100 * correct / total\n",
    "\n",
    "            print('Train  accuracy and loss of the model on  {} images: {} %, {:.5f}'\n",
    "                  .format(len(train_data), epoch_acc, loss))\n",
    "            print('-' * 10)\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "            time_elapsed // 60, time_elapsed % 60))\n",
    "        print('Best validation Acc: {:4f}'.format(best_acc)) \n",
    "        \n",
    "        mean_acc = np.mean(acc_vect)\n",
    "        print('Average accuracy on the validation {} images: {}'\n",
    "              .format(len(train_data),mean_acc))\n",
    "        print('-' * 10)\n",
    "        return best_model, mean_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader):\n",
    "    print(\"Starting testing...\\n\")\n",
    "    model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        test_loss_vect=[]\n",
    "        test_acc_vect=[]\n",
    "        \n",
    "        since = time.time()  #record the beginning time\n",
    "        \n",
    "        for i in range(10):\n",
    "            \n",
    "            Indx = torch.randperm(len(test_data))\n",
    "            Cut=int(len(Indx)/10) # Here 10% showing the proportion of data is chosen for pooling\n",
    "            indices=Indx[:Cut]            \n",
    "            Sampler = Data.SubsetRandomSampler(indices)\n",
    "            pooled_data =  torch.utils.data.DataLoader(test_data , batch_size=BchSz,sampler=Sampler)\n",
    "\n",
    "            for images, labels in pooled_data:\n",
    "                images = Variable(images).cuda()\n",
    "                labels = Variable(labels).cuda()\n",
    "                \n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "            test_loss= loss / total\n",
    "            test_accuracy= 100 * correct / total\n",
    "            \n",
    "            test_loss_vect.append(test_loss)\n",
    "            test_acc_vect.append(test_accuracy)\n",
    "\n",
    "            \n",
    "#             print('Test accuracy and loss for the {}th pool: {:.2f} %, {:.5f}'\n",
    "#                   .format(i+1, test_accuracy, test_loss))\n",
    "            \n",
    "        \n",
    "        mean_test_loss = torch.mean(torch.tensor(test_loss_vect))\n",
    "        mean_test_acc = torch.mean(torch.tensor(test_acc_vect))\n",
    "        std_test_acc = torch.std(torch.tensor(test_acc_vect))\n",
    "        \n",
    "        print('-' * 10)\n",
    "        print('Average test accuracy on test data: {:.2f} %, loss: {:.5f}, Standard deviion of accuracy: {:.4f}'\n",
    "              .format(mean_test_acc, mean_test_loss, std_test_acc))\n",
    "        \n",
    "        print('-' * 10)\n",
    "        time_elapsed = time.time() - since\n",
    "        print('Testing complete in {:.1f}m {:.4f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "        \n",
    "        print('-' * 10)\n",
    "        \n",
    "        return mean_test_acc, mean_test_loss, std_test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode of transformation\n",
    "transformation = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ColorJitter(),\n",
    "    transforms.ToTensor(),\n",
    "]) \n",
    "transformation2 = transforms.Compose([\n",
    "    transforms.ToTensor(),  \n",
    "]) \n",
    "\n",
    "\n",
    "train_data = torchvision.datasets.ImageFolder(train_path,transform=transformation)\n",
    "train_loader =torch.utils.data.DataLoader(train_data, batch_size=BchSz, shuffle=True,\n",
    "                                          num_workers=8)\n",
    "\n",
    "valid_data = torchvision.datasets.ImageFolder(valid_path,transform=transformation)\n",
    "valid_loader =torch.utils.data.DataLoader(valid_data, batch_size=BchSz, shuffle=True,\n",
    "                                          num_workers=8)\n",
    "\n",
    "test_data = torchvision.datasets.ImageFolder(test_path,transform=transformation2)\n",
    "test_loader =torch.utils.data.DataLoader(test_data, batch_size=BchSz, shuffle=True,\n",
    "                                          num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=7, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(16, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=12616704, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## Defining model with different variables, namely:\n",
    "model = ResNet(BasicBlock, [1, 2] , Num_Filter1 , Num_Filter2, Ker_Sz1, Ker_Sz2)      \n",
    "model = model.cuda()\n",
    "print(model)\n",
    "\n",
    "# Defining optimizer with variable learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "optimizer.scheduler=lr_scheduler.ReduceLROnPlateau(optimizer, 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25704274"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-08 17:58:06 Starting training and validation...\n",
      "\n",
      "====================Data and Hyperparameter Overview====================\n",
      "\n",
      "Number of training examples: 12000 , Number of validation examples: 4000 \n",
      "\n",
      "Dropout:0.20, Learning rate: 0.00010 \n",
      "Batch size: 1, Number of epochs: 10 \n",
      "Number of parameter in the model: 25704274\n",
      "================================Results...==============================\n",
      "\n",
      "Epoch [1/10], Step [1000/12000], Loss: 0.0000\n",
      "Epoch [1/10], Step [2000/12000], Loss: 300.5869\n",
      "Epoch [1/10], Step [3000/12000], Loss: 80.5591\n",
      "Epoch [1/10], Step [4000/12000], Loss: 61.7886\n",
      "Epoch [1/10], Step [5000/12000], Loss: 183.3763\n",
      "Epoch [1/10], Step [6000/12000], Loss: 0.0000\n",
      "Epoch [1/10], Step [7000/12000], Loss: 7.6211\n",
      "Epoch [1/10], Step [8000/12000], Loss: 37.4562\n",
      "Epoch [1/10], Step [9000/12000], Loss: 77.3143\n",
      "Epoch [1/10], Step [10000/12000], Loss: 0.0000\n",
      "Epoch [1/10], Step [11000/12000], Loss: 0.0000\n",
      "Epoch [1/10], Step [12000/12000], Loss: 0.0000\n",
      "Validation accuracy and loss of the model on  4000 images: 58.75 %, 0.00000\n",
      "Train  accuracy and loss of the model on  12000 images: 61.858333333333334 %, 0.00000\n",
      "----------\n",
      "Epoch [2/10], Step [1000/12000], Loss: 0.8598\n",
      "Epoch [2/10], Step [2000/12000], Loss: 0.5305\n",
      "Epoch [2/10], Step [3000/12000], Loss: 0.6697\n",
      "Epoch [2/10], Step [4000/12000], Loss: 0.8395\n",
      "Epoch [2/10], Step [5000/12000], Loss: 0.6282\n",
      "Epoch [2/10], Step [6000/12000], Loss: 0.6245\n",
      "Epoch [2/10], Step [7000/12000], Loss: 0.5573\n",
      "Epoch [2/10], Step [8000/12000], Loss: 0.6722\n",
      "Epoch [2/10], Step [9000/12000], Loss: 0.9034\n",
      "Epoch [2/10], Step [10000/12000], Loss: 0.8851\n",
      "Epoch [2/10], Step [11000/12000], Loss: 0.7024\n",
      "Epoch [2/10], Step [12000/12000], Loss: 0.6724\n",
      "Validation accuracy and loss of the model on  4000 images: 52.125 %, 1.39498\n",
      "Train  accuracy and loss of the model on  12000 images: 54.4 %, 1.39913\n",
      "----------\n",
      "Epoch [3/10], Step [1000/12000], Loss: 0.6699\n",
      "Epoch [3/10], Step [2000/12000], Loss: 0.3754\n",
      "Epoch [3/10], Step [3000/12000], Loss: 0.2867\n",
      "Epoch [3/10], Step [4000/12000], Loss: 0.6827\n",
      "Epoch [3/10], Step [5000/12000], Loss: 1.0130\n",
      "Epoch [3/10], Step [6000/12000], Loss: 0.7218\n",
      "Epoch [3/10], Step [7000/12000], Loss: 0.1650\n",
      "Epoch [3/10], Step [8000/12000], Loss: 0.6579\n",
      "Epoch [3/10], Step [9000/12000], Loss: 0.6465\n",
      "Epoch [3/10], Step [10000/12000], Loss: 0.3238\n",
      "Epoch [3/10], Step [11000/12000], Loss: 0.6705\n",
      "Epoch [3/10], Step [12000/12000], Loss: 0.6267\n",
      "Validation accuracy and loss of the model on  4000 images: 54.775 %, 1.60638\n",
      "Train  accuracy and loss of the model on  12000 images: 57.75833333333333 %, 1.11824\n",
      "----------\n",
      "Epoch [4/10], Step [1000/12000], Loss: 0.7133\n",
      "Epoch [4/10], Step [2000/12000], Loss: 0.6051\n",
      "Epoch [4/10], Step [3000/12000], Loss: 0.7134\n",
      "Epoch [4/10], Step [4000/12000], Loss: 0.7690\n",
      "Epoch [4/10], Step [5000/12000], Loss: 0.2603\n",
      "Epoch [4/10], Step [6000/12000], Loss: 0.8750\n",
      "Epoch [4/10], Step [7000/12000], Loss: 0.7542\n",
      "Epoch [4/10], Step [8000/12000], Loss: 0.6825\n",
      "Epoch [4/10], Step [9000/12000], Loss: 0.6603\n",
      "Epoch [4/10], Step [10000/12000], Loss: 0.4995\n",
      "Epoch [4/10], Step [11000/12000], Loss: 0.8621\n",
      "Epoch [4/10], Step [12000/12000], Loss: 0.8141\n",
      "Validation accuracy and loss of the model on  4000 images: 52.575 %, 1.07534\n",
      "Train  accuracy and loss of the model on  12000 images: 57.358333333333334 %, 1.65432\n",
      "----------\n",
      "Epoch [5/10], Step [1000/12000], Loss: 0.7743\n",
      "Epoch [5/10], Step [2000/12000], Loss: 0.8687\n",
      "Epoch [5/10], Step [3000/12000], Loss: 0.6709\n",
      "Epoch [5/10], Step [4000/12000], Loss: 0.7523\n",
      "Epoch [5/10], Step [5000/12000], Loss: 0.6166\n",
      "Epoch [5/10], Step [6000/12000], Loss: 0.6448\n",
      "Epoch [5/10], Step [7000/12000], Loss: 0.5505\n",
      "Epoch [5/10], Step [8000/12000], Loss: 0.8880\n",
      "Epoch [5/10], Step [9000/12000], Loss: 0.7447\n",
      "Epoch [5/10], Step [10000/12000], Loss: 0.6056\n",
      "Epoch [5/10], Step [11000/12000], Loss: 0.1985\n",
      "Epoch [5/10], Step [12000/12000], Loss: 1.0832\n",
      "Validation accuracy and loss of the model on  4000 images: 55.825 %, 1.25189\n",
      "Train  accuracy and loss of the model on  12000 images: 58.666666666666664 %, 1.25140\n",
      "----------\n",
      "Epoch [6/10], Step [1000/12000], Loss: 0.5734\n",
      "Epoch [6/10], Step [2000/12000], Loss: 0.6671\n",
      "Epoch [6/10], Step [3000/12000], Loss: 0.9225\n",
      "Epoch [6/10], Step [4000/12000], Loss: 0.2992\n",
      "Epoch [6/10], Step [5000/12000], Loss: 1.2206\n",
      "Epoch [6/10], Step [6000/12000], Loss: 1.2467\n",
      "Epoch [6/10], Step [7000/12000], Loss: 0.8343\n",
      "Epoch [6/10], Step [8000/12000], Loss: 0.4752\n",
      "Epoch [6/10], Step [9000/12000], Loss: 0.5015\n",
      "Epoch [6/10], Step [10000/12000], Loss: 0.7835\n",
      "Epoch [6/10], Step [11000/12000], Loss: 0.5814\n",
      "Epoch [6/10], Step [12000/12000], Loss: 0.6272\n",
      "Validation accuracy and loss of the model on  4000 images: 56.2 %, 1.40135\n",
      "Train  accuracy and loss of the model on  12000 images: 57.516666666666666 %, 1.28384\n",
      "----------\n",
      "Epoch [7/10], Step [1000/12000], Loss: 0.7358\n",
      "Epoch [7/10], Step [2000/12000], Loss: 1.3603\n",
      "Epoch [7/10], Step [3000/12000], Loss: 0.5795\n",
      "Epoch [7/10], Step [4000/12000], Loss: 0.6834\n",
      "Epoch [7/10], Step [5000/12000], Loss: 1.0554\n",
      "Epoch [7/10], Step [6000/12000], Loss: 0.6912\n",
      "Epoch [7/10], Step [7000/12000], Loss: 0.6563\n",
      "Epoch [7/10], Step [8000/12000], Loss: 0.7576\n",
      "Epoch [7/10], Step [9000/12000], Loss: 0.7538\n",
      "Epoch [7/10], Step [10000/12000], Loss: 0.8305\n",
      "Epoch [7/10], Step [11000/12000], Loss: 0.4665\n",
      "Epoch [7/10], Step [12000/12000], Loss: 0.6119\n",
      "Validation accuracy and loss of the model on  4000 images: 56.2 %, 1.48244\n",
      "Train  accuracy and loss of the model on  12000 images: 57.43333333333333 %, 0.93786\n",
      "----------\n",
      "Epoch [8/10], Step [1000/12000], Loss: 0.5478\n",
      "Epoch [8/10], Step [2000/12000], Loss: 0.5227\n",
      "Epoch [8/10], Step [3000/12000], Loss: 0.7137\n",
      "Epoch [8/10], Step [4000/12000], Loss: 0.5344\n",
      "Epoch [8/10], Step [5000/12000], Loss: 0.6673\n",
      "Epoch [8/10], Step [6000/12000], Loss: 0.6529\n",
      "Epoch [8/10], Step [7000/12000], Loss: 0.8201\n",
      "Epoch [8/10], Step [8000/12000], Loss: 0.8356\n",
      "Epoch [8/10], Step [9000/12000], Loss: 0.7158\n",
      "Epoch [8/10], Step [10000/12000], Loss: 0.4191\n",
      "Epoch [8/10], Step [11000/12000], Loss: 0.6306\n",
      "Epoch [8/10], Step [12000/12000], Loss: 0.7051\n",
      "Validation accuracy and loss of the model on  4000 images: 55.975 %, 1.15222\n",
      "Train  accuracy and loss of the model on  12000 images: 58.65833333333333 %, 1.17476\n",
      "----------\n",
      "Epoch [9/10], Step [1000/12000], Loss: 0.4424\n",
      "Epoch [9/10], Step [2000/12000], Loss: 0.8872\n",
      "Epoch [9/10], Step [3000/12000], Loss: 0.7984\n",
      "Epoch [9/10], Step [4000/12000], Loss: 0.7532\n",
      "Epoch [9/10], Step [5000/12000], Loss: 0.7731\n",
      "Epoch [9/10], Step [6000/12000], Loss: 0.9018\n",
      "Epoch [9/10], Step [7000/12000], Loss: 1.0036\n",
      "Epoch [9/10], Step [8000/12000], Loss: 0.8370\n",
      "Epoch [9/10], Step [9000/12000], Loss: 0.1235\n",
      "Epoch [9/10], Step [10000/12000], Loss: 0.7615\n",
      "Epoch [9/10], Step [11000/12000], Loss: 0.7638\n",
      "Epoch [9/10], Step [12000/12000], Loss: 0.1504\n",
      "Validation accuracy and loss of the model on  4000 images: 63.5 %, 1.07168\n",
      "Train  accuracy and loss of the model on  12000 images: 64.44166666666666 %, 0.23583\n",
      "----------\n",
      "Epoch [10/10], Step [1000/12000], Loss: 0.3102\n",
      "Epoch [10/10], Step [2000/12000], Loss: 0.2120\n",
      "Epoch [10/10], Step [3000/12000], Loss: 1.2921\n",
      "Epoch [10/10], Step [4000/12000], Loss: 0.5197\n",
      "Epoch [10/10], Step [5000/12000], Loss: 0.8205\n",
      "Epoch [10/10], Step [6000/12000], Loss: 0.8574\n",
      "Epoch [10/10], Step [7000/12000], Loss: 0.8023\n",
      "Epoch [10/10], Step [8000/12000], Loss: 0.5594\n",
      "Epoch [10/10], Step [9000/12000], Loss: 0.7509\n",
      "Epoch [10/10], Step [10000/12000], Loss: 0.3874\n",
      "Epoch [10/10], Step [11000/12000], Loss: 0.6733\n",
      "Epoch [10/10], Step [12000/12000], Loss: 0.5383\n",
      "Validation accuracy and loss of the model on  4000 images: 63.325 %, 1.47602\n",
      "Train  accuracy and loss of the model on  12000 images: 65.16666666666667 %, 1.37828\n",
      "----------\n",
      "Training complete in 256m 42s\n",
      "Best validation Acc: 63.500000\n",
      "Average accuracy on the validation 12000 images: 56.925\n",
      "----------\n",
      "Starting testing...\n",
      "\n",
      "----------\n",
      "Average test accuracy on test data: 66.02 %, loss: 0.00042, Standard deviion of accuracy: 1.2713\n",
      "----------\n",
      "Testing complete in 2.0m 9.2043s\n",
      "----------\n",
      "2020-01-08 22:16:57 Starting training and validation...\n",
      "\n",
      "====================Data and Hyperparameter Overview====================\n",
      "\n",
      "Number of training examples: 12000 , Number of validation examples: 4000 \n",
      "\n",
      "Dropout:0.20, Learning rate: 0.00010 \n",
      "Batch size: 1, Number of epochs: 10 \n",
      "Number of parameter in the model: 25704274\n",
      "================================Results...==============================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [1000/12000], Loss: 0.5723\n",
      "Epoch [1/10], Step [2000/12000], Loss: 0.3111\n",
      "Epoch [1/10], Step [3000/12000], Loss: 0.8324\n",
      "Epoch [1/10], Step [4000/12000], Loss: 1.0623\n",
      "Epoch [1/10], Step [5000/12000], Loss: 0.0215\n",
      "Epoch [1/10], Step [6000/12000], Loss: 0.9125\n",
      "Epoch [1/10], Step [7000/12000], Loss: 0.4944\n",
      "Epoch [1/10], Step [8000/12000], Loss: 0.2904\n",
      "Epoch [1/10], Step [9000/12000], Loss: 0.4409\n",
      "Epoch [1/10], Step [10000/12000], Loss: 0.7368\n",
      "Epoch [1/10], Step [11000/12000], Loss: 0.7483\n",
      "Epoch [1/10], Step [12000/12000], Loss: 0.7491\n",
      "Validation accuracy and loss of the model on  4000 images: 62.775 %, 1.16847\n",
      "Train  accuracy and loss of the model on  12000 images: 65.00833333333334 %, 1.03797\n",
      "----------\n",
      "Epoch [2/10], Step [1000/12000], Loss: 0.8921\n",
      "Epoch [2/10], Step [2000/12000], Loss: 0.7256\n",
      "Epoch [2/10], Step [3000/12000], Loss: 0.4678\n",
      "Epoch [2/10], Step [4000/12000], Loss: 0.5304\n",
      "Epoch [2/10], Step [5000/12000], Loss: 0.5473\n",
      "Epoch [2/10], Step [6000/12000], Loss: 0.5151\n",
      "Epoch [2/10], Step [7000/12000], Loss: 0.9006\n",
      "Epoch [2/10], Step [8000/12000], Loss: 0.8369\n",
      "Epoch [2/10], Step [9000/12000], Loss: 0.8867\n",
      "Epoch [2/10], Step [10000/12000], Loss: 0.4713\n",
      "Epoch [2/10], Step [11000/12000], Loss: 0.1460\n",
      "Epoch [2/10], Step [12000/12000], Loss: 0.8795\n",
      "Validation accuracy and loss of the model on  4000 images: 62.1 %, 1.80494\n"
     ]
    }
   ],
   "source": [
    "seed= [1, 3, 7, 19, 22]\n",
    "#seed= [22]\n",
    "\n",
    "val_acc_vect=[]\n",
    "test_acc_vect=[]\n",
    "\n",
    "\n",
    "for ii in seed: \n",
    "    torch.cuda.manual_seed(ii)\n",
    "    torch.manual_seed(ii)\n",
    "    \n",
    "    model, val_acc= train_model(model, criterion, optimizer,  Dropout, learning_rate,  BchSz, EPOCH)\n",
    "    testing = test_model (model, test_loader)\n",
    "    test_acc= testing[0]\n",
    "    \n",
    "    \n",
    "    val_acc_vect.append( val_acc )\n",
    "    test_acc_vect.append(test_acc)\n",
    "    \n",
    "    mean_val_acc = torch.mean(torch.tensor(val_acc_vect))\n",
    "    mean_test_acc = torch.mean(torch.tensor(test_acc_vect))\n",
    "    \n",
    "    \n",
    "print('-' * 10)\n",
    "print('-' * 10)\n",
    "print('Average of validation accuracies on 5 different random seed: {:.2f} %, Average of testing accuracies on 5 different random seed: {:.2f} %'\n",
    "      .format(mean_val_acc, mean_test_acc)) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
